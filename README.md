**é¦–å…ˆï¼Œ**å…ˆç¥ˆæ±‚è‡ªå·±ä¸€æ¬¡æ€§éƒ¨ç½²æˆåŠŸï¼(ä½†æ˜¯å¾€å¾€äº‹ä¸Žæ„¿è¿ï¼Œåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œä¸€å¤§å †çš„çŽ¯å¢ƒåŠä»¥æ¥ç¨‹åºçš„å®‰è£…ï¼Œèƒ½è®©ä½ ç–¯åˆ°å§¥å§¥å®¶ï¼Œä¸ä¿¡ä½ è·Ÿç€æˆ‘ä¸€èµ·è¯•è¯•ï¼‰
```
æˆ‘æ²¡æœ‰å†™è¿‡github.comçš„è¯´æ˜Žè§„èŒƒï¼Œçœ‹èµ·æ¥å¾ˆä¸è§„åˆ™ä¹Ÿæ˜¯æ­£å¸¸çš„ï¼Œæ…¢æ…¢é€‚åº”å°±å¯ä»¥äº†
```



#æœ¬éƒ¨ç½²å‚è€ƒï¼š

### https://github.com/natlamir/tpsm

### https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model


### windows 10ç³»ç»Ÿ ä¸“ä¸šç‰ˆ å®žè·µæ­¥éª¤ï¼š**
  ### 1.æ‡‚å¾—ä¸Šç½‘ï¼šå¦‚æžœä¸æ‡‚ä¸Šç½‘ï¼Œä½ ä¼šæ›´åŠ çš„æŠ“ç‹‚ï¼Œå› ä¸ºä½ çš„å¤§é‡ä¾èµ–åº“ï¼Œéƒ½æ˜¯ä¼šè¢«å‘ŠçŸ¥æ— æ³•ä¸‹è½½ï¼Œå“ˆå“ˆå“ˆå“ˆ~~~ï¼Œå½“ç„¶ï¼Œèƒ½åŽŸç”Ÿçœ‹åˆ°æœ¬æ–‡ä»¶çš„äººï¼Œåº”è¯¥éƒ½æ‡‚å¾—ä¸Šç½‘äº†å§

  ### 2.ç¡¬ä»¶åŸºç¡€ï¼šéœ€è¦æœ‰ä¸€å¼ Nç³»åˆ—æ˜¾å¡ï¼Œæœ¬äººç”¨äºŽæµ‹è¯•çš„æ˜¯RTX3060 12Gçš„æ˜¾å¡

  ### 3.é©±åŠ¨è½¯ä»¶ä¾èµ–ï¼šå®‰è£…æ˜¾å¡é©±åŠ¨å’Œå·¥å…·åŒ…ï¼šCUDAï¼Œè‡ªå®šåœ¨ https://www.nvidia.cn/Download/Find.aspx?lang=cn æ ¹æ®å®žé™…ä¸‹è½½å®‰è£…

  ### 4.å¤šè™šæ‹ŸçŽ¯å¢ƒé…ç½®ç®¡ç†å·¥å…·Anaconda3ï¼šç”¨æˆ·Anaconda(condaå‘½ä»¤)æ¥åˆ›å»ºä¸€ä¸ªè™šæ‹ŸçŽ¯å¢ƒï¼Œå¯ä»¥ç»™è¿™ä¸ªè™šæ‹ŸçŽ¯å¢ƒå‘½åï¼Œåˆ‡æ¢åˆ°è¯¥è™šæ‹ŸçŽ¯å¢ƒåŽï¼Œç„¶åŽåœ¨è™šæ‹ŸçŽ¯å¢ƒå®‰è£…æ‰€æœ‰çš„ç¨‹åºä»¥åŠå¯¹åº”çš„ä¾èµ–ï¼Œåœ¨è¿è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°±ä¼šè‡ªç„¶å¼•ç”¨è¯¥è™šæ‹ŸçŽ¯å¢ƒä¸‹çš„å˜é‡ã€å‘½ä»¤å’Œé…ç½®ï¼Œå®Œç¾Žè§£å†³ä¸€å°ç³»ç»Ÿä¸‹å¯ä»¥å¤šä¸ªä¸åŒçŽ¯å¢ƒçš„ç›®æ ‡ï¼›


  tips:Anaconda3ï¼Œå…·æœ‰ä¸€å°ç³»ç»Ÿæ­å»ºå¤šä¸ªä¸æ‡‚è¿è¡ŒçŽ¯å¢ƒçš„éœ€æ±‚ï¼Œæ¯”å¦‚ä½ ç”µè„‘é‡ŒåŽŸæ¥è¿è¡Œpython2çš„ç¨‹åºï¼ŒçŽ°åœ¨éœ€è¦å®‰è£…python3.9æ¥è¿è¡Œæœ¬ç¨‹åºï¼Œé‚£ä¹ˆä½ åœ¨ä¸æ‡‚condaç®¡ç†çš„æ—¶å€™ï¼Œç¬¬ä¸€æƒ³æ³•æ˜¯å§python2ç»™å‡çº§ä¸ºpython3.9,ç„¶åŽä½ å¯èƒ½è¦æŠ“ç‹‚çš„æ˜¯ï¼Œå½“ä½ å®‰è£…å®Œæˆpython3.9åŽï¼Œå‘çŽ°åŽŸæ¥python2çš„ç¨‹åºï¼Œéƒ½æ— æ³•æ­£å¸¸è¿è¡Œäº†ï¼Œç„¶åŽä½ åˆç»§ç»­æŠŠpython3.9ç»™å¸è½½é‡æ–°å®‰è£…python2,ç„¶åŽä½ å°±æ²¡æœ‰ç„¶åŽï¼Œç„¶åŽä½ å°±æƒ³æ”¾å¼ƒæœ¬æ¬¡å®‰è£…ï¼Œæœ‰ç§æ—¥äº†dogçš„æ„Ÿè§‰å§
ä¸è¿‡æ²¡æœ‰å…³ç³»ï¼Œåªè¦ä½ ç¨å¾®äº†è§£ä¸‹Anaconda,å°±èƒ½è§£å†³ä½ è¦ä½¿ç”¨python3.9çš„ç›®æ ‡äº†ï¼›

### å®‰è£…æ­¥éª¤ï¼šï¼ˆæœ€å¥½ä¸è¦å®‰è£…åœ¨Cç›˜ï¼Œå®‰è£…ç¨‹åºå°½é‡ä¸è¦åˆç©ºæ ¼çš„ç›®å½•ï¼‰
1.å®‰è£…Anaconda3ï¼Œè‡ªè¡ŒåŽ»å®˜ç½‘ä¸‹è½½å®‰è£…ï¼Œç„¶åŽå¯ä»¥èœå•é€šè¿‡å®ƒçš„powerShell Promptï¼Œä¹Ÿå¯ä»¥ç›´æŽ¥cmdï¼Œç„¶åŽcdåˆ°ä½ éœ€è¦å®‰è£…è¯¥ç¨‹åºçš„ç›®å½•ä¸‹ï¼Œæˆ‘çš„æ˜¯H:\AIç›®å½•
2.åˆ›å»ºçŽ¯å¢ƒ
```
#åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
conda create --name tpsm python=3.9 -y
#åˆ‡æ¢åˆ°è™šæ‹ŸçŽ¯å¢ƒ
conda activate tpsm
ä¸‹è½½æ¨¡åž‹ç¨‹åº

git clone https://github.com/ym6789/tpsm.git tpsm
cd tpsm
```

3.å®‰è£…PyTorch(æˆ‘ä¹Ÿä¸çŸ¥é“è¿™æ˜¯å•¥ï¼Œåæ­£åŽé¢è¿™ä¸ªæ˜¯æœ€æŠ˜è…¾äººçš„)
 ```
 conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y
ï¼ˆè¿™é‡Œæˆ‘ä¹Ÿæžä¸æ‡‚ï¼Œæˆ‘è¿™è¾¹çš„cudaç‰ˆæœ¬æ˜¯12.3,ä¹Ÿä¸çŸ¥é“æ˜¯ä¸æ˜¯è¿™é‡Œå‡ºé—®é¢˜äº†ï¼‰

å®‰è£…å®ŒæˆåŽï¼Œæˆ‘æŸ¥çœ‹ä¸‹æ˜¯è¿™æ ·çš„ï¼š
(tpsm) PS H:\AI\tpsm> pip list
Package            Version
------------------ ----------
Brotli             1.0.9
certifi            2023.11.17
cffi               1.16.0
charset-normalizer 2.0.4
cryptography       41.0.7
filelock           3.13.1
gmpy2              2.1.2
idna               3.4
Jinja2             3.1.2
MarkupSafe         2.1.3
mkl-fft            1.3.8
mkl-random         1.2.4
mkl-service        2.4.0
mpmath             1.3.0
networkx           3.1
numpy              1.26.3
Pillow             10.0.1
pip                23.3.1
pycparser          2.21
pyOpenSSL          23.2.0
PySocks            1.7.1
PyYAML             6.0.1
requests           2.31.0
setuptools         68.2.2
sympy              1.12
torch              2.1.2
torchaudio         2.1.2
torchvision        0.16.2
typing_extensions  4.9.0
urllib3            1.26.18
wheel              0.41.2
win-inet-pton      1.1.0




å®‰è£…requirements.txtæ–‡ä»¶çš„ä¾èµ–ç»„ä»¶
(tpsm) PS H:\AI\tpsm> pip install -r requirements.txt

(tpsm) PS H:\AI\tpsm> pip list
Package            Version
------------------ ----------
Brotli             1.0.9
certifi            2023.11.17
cffi               1.14.6
charset-normalizer 2.0.4
colorama           0.4.6
cryptography       41.0.7
cycler             0.10.0
decorator          5.1.0
face-alignment     1.3.5
filelock           3.13.1
fsspec             2023.12.2
gmpy2              2.1.2
idna               3.4
imageio            2.9.0
imageio-ffmpeg     0.4.5
Jinja2             3.1.2
joblib             1.3.2
kiwisolver         1.3.2
llvmlite           0.39.1
MarkupSafe         2.1.3
matplotlib         3.4.3
mkl-fft            1.3.8
mkl-random         1.2.4
mkl-service        2.4.0
mpmath             1.3.0
networkx           2.6.3
numba              0.56.4
numpy              1.20.3
opencv-python      4.9.0.80
pandas             1.3.3
Pillow             9.2.0
pip                23.3.1
pycparser          2.20
pyOpenSSL          23.2.0
pyparsing          2.4.7
PySocks            1.7.1
python-dateutil    2.8.2
pytz               2021.1
PyWavelets         1.1.1
PyYAML             5.4.1
requests           2.31.0
scikit-image       0.18.3
scikit-learn       1.0
scipy              1.7.1
setuptools         68.2.2
six                1.16.0
sympy              1.12
threadpoolctl      3.2.0
tifffile           2023.12.9
torch              2.1.2
torchaudio         2.1.2
torchvision        0.16.2
tqdm               4.62.3
typing_extensions  4.9.0
urllib3            1.26.18
wheel              0.41.2
win-inet-pton      1.1.0
(tpsm) PS H:\AI\tpsm>




```
# æµ‹è¯•pytorchå®‰è£…æ˜¯å¦æˆåŠŸï¼š
```
import torch

print(torch.__version__)

è¾“å‡ºï¼š2.1.2ï¼ˆè¿™ä¸ªç‰ˆæœ¬åŽç»­å¯èƒ½ä¼šåˆä¸€å¤§å †çš„æŠ¥é”™ï¼Œé‚£ä¹Ÿæ²¡å…³ç³»ï¼Œå…ˆç»§ç»­å®žæ–½çœ‹ï¼‰

```

### ä¸‹è½½æ¨¡åž‹æ•°æ®é›† Datasets
tips:å…·ä½“æ¯ä¸€ä¸ªæ•°æ®é›†ä¸»è¦åå‘ä¸Žé‚£ä¸ªé¢†åŸŸï¼Œæœªç ”ç©¶

1) **MGif**. Follow [Monkey-Net](https://github.com/AliaksandrSiarohin/monkey-net).

2) **TaiChiHD** and **VoxCeleb**. Follow instructions from [video-preprocessing](https://github.com/AliaksandrSiarohin/video-preprocessing). 

3) **TED-talks**. Follow instructions from [MRAA](https://github.com/snap-research/articulated-animation).

Here are **VoxCeleb**, **TaiChiHD** and **TED-talks**  pre-processed datasets used in the paper. [Baidu Yun](https://pan.baidu.com/s/1HKJOtXBIiP_tlLiFbzn3oA?pwd=x7xv)
Download all files under the folder, then merge the files and decompress, for example:
```bash

cat vox.tar.* > vox.tar
tar xvf vox.tar

ä¸‹è½½åŽï¼ŒæŠŠæ–‡ä»¶è§£åŽ‹åˆ°H:\AI\tpsm\checkpoints

(tpsm) PS H:\AI\tpsm\checkpoints> dir
Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a----         2024/1/12     20:26     1359827700 drive-download-20240112T122126Z-001.zip
------          2022/5/2     16:27      306156401 mgif.pth.tar
------          2022/5/2     16:27      350993469 taichi.pth.tar
------          2022/5/2     16:27      350993469 ted.pth.tar
------          2023/7/8      2:04         667037 Thin-Plate-Spline-Motion-Model.ipynb
------          2022/5/2     16:27      350993469 vox.pth.tar


```
# ç´ æå‡†å¤‡ï¼šæ‰¾ä¸€ä¸ªæŸäººçš„ä¸€å¯¸å¤´åƒç…§ç‰‡(png)ï¼Œæ”¾åœ¨assetsæ–‡ä»¶å¤¹ä¸‹ï¼Œæ›¿æ¢source.pngæ–‡ä»¶

# å¼€å§‹æ‰§è¡Œå‘½ä»¤ï¼šï¼ˆåˆ°è¿™é‡ŒåŽï¼Œåº”è¯¥ä¼šè§‰å¾—æŠ¥é”™ï¼Œè¿™ä¸ªæ˜¯å®‰è£…å¤§é‡çš„githubé¡¹ç›®å‘Šè¯‰æˆ‘çš„ç»éªŒï¼Œè™½ç„¶è¿˜æ²¡æœ‰æ‰§è¡Œï¼‰
```
python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image assets/source.png --driving_video assets/driving.mp4
```

```
H:\AI\Anaconda3\envs\tpsm\python.exe: can't open file 'H:\AI\tpsm\checkpoints\demo.py': [Errno 2] No such file or directory
(tpsm) PS H:\AI\tpsm\checkpoints> cd ..
(tpsm) PS H:\AI\tpsm> python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image assets/source.png --driving_video assets/driving.mp4
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torch\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\TensorShape.cpp:3527.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
  0%|                                                                                                                                                                                                               | 0/169 [00:00<?, ?it/s]AttributeError: 'NoneType' object has no attribute 'close'
Exception ignored in: 'scipy.spatial.qhull._Qhull.__dealloc__'
Traceback (most recent call last):
  File "H:\AI\tpsm\demo.py", line 25, in relative_kp
    source_area = ConvexHull(kp_source['fg_kp'][0].data.cpu().numpy()).volume
AttributeError: 'NoneType' object has no attribute 'close'
  0%|                                                                                                                                                                                                               | 0/169 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "H:\AI\tpsm\demo.py", line 179, in <module>
    predictions = make_animation(source_image, driving_video, inpainting, kp_detector, dense_motion_network, avd_network, device = device, mode = opt.mode)
  File "H:\AI\tpsm\demo.py", line 86, in make_animation
    kp_norm = relative_kp(kp_source=kp_source, kp_driving=kp_driving,
  File "H:\AI\tpsm\demo.py", line 25, in relative_kp
    source_area = ConvexHull(kp_source['fg_kp'][0].data.cpu().numpy()).volume
  File "qhull.pyx", line 2434, in scipy.spatial.qhull.ConvexHull.__init__
  File "qhull.pyx", line 269, in scipy.spatial.qhull._Qhull.__init__
  File "messagestream.pyx", line 36, in scipy._lib.messagestream.MessageStream.__init__
OSError: Failed to open file b'C:\\Users\\\xe5\x8d\x8e\xe5\xb8\x88\xe7\xa7\x91\xe6\x95\x99\\AppData\\Local\\Temp\\scipy-hh_y9mwf'

(tpsm) PS H:\AI\tpsm>
æŸ¥è¯¢äº†ä¸‹ï¼Œç„¶åŽå‘ŠçŸ¥è¯´å› ä¸ºæœ‰ä¸­æ–‡è·¯å¾„ï¼Œä¿®æ”¹äº†çŽ¯å¢ƒå˜é‡ä¸­çš„tempï¼Œæ‰€ä»¥å‚è€ƒé“¾æŽ¥https://blog.csdn.net/datao3022/article/details/109186403
å³é”®ç‚¹å‡»è®¡ç®—æœº -> å±žæ€§ -> é«˜çº§ç³»ç»Ÿè®¾ç½® -> çŽ¯å¢ƒå˜é‡

å…³é—­å‘½ä»¤çª—å£é‡å¯ï¼ˆå¦‚æžœä¸è¡Œï¼Œå°±é‡å¯ç”µè„‘å†è¯•è¯•ï¼‰



```






# [CVPR2022] Thin-Plate Spline Motion Model for Image Animation

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
![stars](https://img.shields.io/github/stars/yoyo-nb/Thin-Plate-Spline-Motion-Model.svg?style=flat)
![GitHub repo size](https://img.shields.io/github/repo-size/yoyo-nb/Thin-Plate-Spline-Motion-Model.svg)

Source code of the CVPR'2022 paper "Thin-Plate Spline Motion Model for Image Animation"

[**Paper**](https://arxiv.org/abs/2203.14367) **|** [**Supp**](https://cloud.tsinghua.edu.cn/f/f7b8573bb5b04583949f/?dl=1)

### Example animation

![vox](assets/vox.gif)
![ted](assets/ted.gif)

**PS**: The paper trains the model for 100 epochs for a fair comparison. You can use more data and train for more epochs to get better performance.


### Web demo for animation
- Integrated into [Huggingface Spaces ðŸ¤—](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)
- Try the web demo for animation here: [![Replicate](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model/badge)](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model)
- Google Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DREfdpnaBhqISg0fuQlAAIwyGVn1loH_?usp=sharing)

### Pre-trained models
- ~~[Tsinghua Cloud](https://cloud.tsinghua.edu.cn/d/30ab8765da364fefa101/)~~
- [Yandex](https://disk.yandex.com/d/bWopgbGj1ZUV1w)
- [Google Drive](https://drive.google.com/drive/folders/1pNDo1ODQIb5HVObRtCmubqJikmR7VVLT?usp=sharing)
- [Baidu Yun](https://pan.baidu.com/s/1hnXmDpIbRC6WqE3tF9c5QA?pwd=1234)

### Installation

We support ```python3```.(Recommended version is Python 3.9).
To install the dependencies run:
```bash
pip install -r requirements.txt
```


### YAML configs
 
There are several configuration files one for each `dataset` in the `config` folder named as ```config/dataset_name.yaml```. 

See description of the parameters in the ```config/taichi-256.yaml```.

### Datasets

1) **MGif**. Follow [Monkey-Net](https://github.com/AliaksandrSiarohin/monkey-net).

2) **TaiChiHD** and **VoxCeleb**. Follow instructions from [video-preprocessing](https://github.com/AliaksandrSiarohin/video-preprocessing). 

3) **TED-talks**. Follow instructions from [MRAA](https://github.com/snap-research/articulated-animation).

Here are **VoxCeleb**, **TaiChiHD** and **TED-talks**  pre-processed datasets used in the paper. [Baidu Yun](https://pan.baidu.com/s/1HKJOtXBIiP_tlLiFbzn3oA?pwd=x7xv)
Download all files under the folder, then merge the files and decompress, for example:
```bash
cat vox.tar.* > vox.tar
tar xvf vox.tar
```


### Training
To train a model on specific dataset run:
```
CUDA_VISIBLE_DEVICES=0,1 python run.py --config config/dataset_name.yaml --device_ids 0,1
```
A log folder named after the timestamp will be created. Checkpoints, loss values, reconstruction results will be saved to this folder.


#### Training AVD network
To train a model on specific dataset run:
```
CUDA_VISIBLE_DEVICES=0 python run.py --mode train_avd --checkpoint '{checkpoint_folder}/checkpoint.pth.tar' --config config/dataset_name.yaml
```
Checkpoints, loss values, reconstruction results will be saved to `{checkpoint_folder}`.



### Evaluation on video reconstruction

To evaluate the reconstruction performance run:
```
CUDA_VISIBLE_DEVICES=0 python run.py --mode reconstruction --config config/dataset_name.yaml --checkpoint '{checkpoint_folder}/checkpoint.pth.tar'
```
The `reconstruction` subfolder will be created in `{checkpoint_folder}`.
The generated video will be stored to this folder, also generated videos will be stored in ```png``` subfolder in loss-less '.png' format for evaluation.
To compute metrics, follow instructions from [pose-evaluation](https://github.com/AliaksandrSiarohin/pose-evaluation).


### Image animation demo
- notebook: `demo.ipynb`, edit the config cell and run for image animation.
- python:
```bash
CUDA_VISIBLE_DEVICES=0 python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image ./source.jpg --driving_video ./driving.mp4
```

# Acknowledgments
The main code is based upon [FOMM](https://github.com/AliaksandrSiarohin/first-order-model) and [MRAA](https://github.com/snap-research/articulated-animation)

Thanks for the excellent works!

And Thanks to:

- [@chenxwh](https://github.com/chenxwh): Add Web Demo & Docker environment [![Replicate](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model/badge)](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model) 

- [@TalkUHulk](https://github.com/TalkUHulk): The C++/Python demo is provided in [Image-Animation-Turbo-Boost](https://github.com/TalkUHulk/Image-Animation-Turbo-Boost)

- [@AK391](https://github.com/AK391): Add huggingface web demo [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)
