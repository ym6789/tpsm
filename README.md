**È¶ñÂÖàÔºå**ÂÖàÁ•àÊ±ÇËá™Â∑±‰∏ÄÊ¨°ÊÄßÈÉ®ÁΩ≤ÊàêÂäüÔºÅ(‰ΩÜÊòØÂæÄÂæÄ‰∫ã‰∏éÊÑøËøùÔºåÂú®ÈÉ®ÁΩ≤ËøáÁ®ã‰∏≠Ôºå‰∏ÄÂ§ßÂ†ÜÁöÑÁéØÂ¢ÉÂèä‰ª•Êù•Á®ãÂ∫èÁöÑÂÆâË£ÖÔºåËÉΩËÆ©‰Ω†ÁñØÂà∞Âß•Âß•ÂÆ∂Ôºå‰∏ç‰ø°‰Ω†Ë∑üÁùÄÊàë‰∏ÄËµ∑ËØïËØïÔºâ
```
ÊàëÊ≤°ÊúâÂÜôËøágithub.comÁöÑËØ¥ÊòéËßÑËåÉÔºåÁúãËµ∑Êù•Âæà‰∏çËßÑÂàô‰πüÊòØÊ≠£Â∏∏ÁöÑÔºåÊÖ¢ÊÖ¢ÈÄÇÂ∫îÂ∞±ÂèØ‰ª•‰∫Ü
```



#Êú¨ÈÉ®ÁΩ≤ÂèÇËÄÉÔºö

### https://github.com/natlamir/tpsm

### https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model


### windows 10Á≥ªÁªü ‰∏ì‰∏öÁâà ÂÆûË∑µÊ≠•È™§Ôºö**
  ### 1.ÊáÇÂæó‰∏äÁΩëÔºöÂ¶ÇÊûú‰∏çÊáÇ‰∏äÁΩëÔºå‰Ω†‰ºöÊõ¥Âä†ÁöÑÊäìÁãÇÔºåÂõ†‰∏∫‰Ω†ÁöÑÂ§ßÈáè‰æùËµñÂ∫ìÔºåÈÉΩÊòØ‰ºöË¢´ÂëäÁü•Êó†Ê≥ï‰∏ãËΩΩÔºåÂìàÂìàÂìàÂìà~~~ÔºåÂΩìÁÑ∂ÔºåËÉΩÂéüÁîüÁúãÂà∞Êú¨Êñá‰ª∂ÁöÑ‰∫∫ÔºåÂ∫îËØ•ÈÉΩÊáÇÂæó‰∏äÁΩë‰∫ÜÂêß

  ### 2.Á°¨‰ª∂Âü∫Á°ÄÔºöÈúÄË¶ÅÊúâ‰∏ÄÂº†NÁ≥ªÂàóÊòæÂç°ÔºåÊú¨‰∫∫Áî®‰∫éÊµãËØïÁöÑÊòØRTX3060 12GÁöÑÊòæÂç°

  ### 3.È©±Âä®ËΩØ‰ª∂‰æùËµñÔºöÂÆâË£ÖÊòæÂç°È©±Âä®ÂíåÂ∑•ÂÖ∑ÂåÖÔºöCUDAÔºåËá™ÂÆöÂú® https://www.nvidia.cn/Download/Find.aspx?lang=cn Ê†πÊçÆÂÆûÈôÖ‰∏ãËΩΩÂÆâË£Ö

  ### 4.Â§öËôöÊãüÁéØÂ¢ÉÈÖçÁΩÆÁÆ°ÁêÜÂ∑•ÂÖ∑Anaconda3ÔºöÁî®Êà∑Anaconda(condaÂëΩ‰ª§)Êù•ÂàõÂª∫‰∏Ä‰∏™ËôöÊãüÁéØÂ¢ÉÔºåÂèØ‰ª•ÁªôËøô‰∏™ËôöÊãüÁéØÂ¢ÉÂëΩÂêçÔºåÂàáÊç¢Âà∞ËØ•ËôöÊãüÁéØÂ¢ÉÂêéÔºåÁÑ∂ÂêéÂú®ËôöÊãüÁéØÂ¢ÉÂÆâË£ÖÊâÄÊúâÁöÑÁ®ãÂ∫è‰ª•ÂèäÂØπÂ∫îÁöÑ‰æùËµñÔºåÂú®ËøêË°åÁöÑËøáÁ®ã‰∏≠ÔºåÂ∞±‰ºöËá™ÁÑ∂ÂºïÁî®ËØ•ËôöÊãüÁéØÂ¢É‰∏ãÁöÑÂèòÈáè„ÄÅÂëΩ‰ª§ÂíåÈÖçÁΩÆÔºåÂÆåÁæéËß£ÂÜ≥‰∏ÄÂè∞Á≥ªÁªü‰∏ãÂèØ‰ª•Â§ö‰∏™‰∏çÂêåÁéØÂ¢ÉÁöÑÁõÆÊ†áÔºõ


  tips:Anaconda3ÔºåÂÖ∑Êúâ‰∏ÄÂè∞Á≥ªÁªüÊê≠Âª∫Â§ö‰∏™‰∏çÊáÇËøêË°åÁéØÂ¢ÉÁöÑÈúÄÊ±ÇÔºåÊØîÂ¶Ç‰Ω†ÁîµËÑëÈáåÂéüÊù•ËøêË°åpython2ÁöÑÁ®ãÂ∫èÔºåÁé∞Âú®ÈúÄË¶ÅÂÆâË£Öpython3.9Êù•ËøêË°åÊú¨Á®ãÂ∫èÔºåÈÇ£‰πà‰Ω†Âú®‰∏çÊáÇcondaÁÆ°ÁêÜÁöÑÊó∂ÂÄôÔºåÁ¨¨‰∏ÄÊÉ≥Ê≥ïÊòØÂêßpython2ÁªôÂçáÁ∫ß‰∏∫python3.9,ÁÑ∂Âêé‰Ω†ÂèØËÉΩË¶ÅÊäìÁãÇÁöÑÊòØÔºåÂΩì‰Ω†ÂÆâË£ÖÂÆåÊàêpython3.9ÂêéÔºåÂèëÁé∞ÂéüÊù•python2ÁöÑÁ®ãÂ∫èÔºåÈÉΩÊó†Ê≥ïÊ≠£Â∏∏ËøêË°å‰∫ÜÔºåÁÑ∂Âêé‰Ω†ÂèàÁªßÁª≠Êääpython3.9ÁªôÂç∏ËΩΩÈáçÊñ∞ÂÆâË£Öpython2,ÁÑ∂Âêé‰Ω†Â∞±Ê≤°ÊúâÁÑ∂ÂêéÔºåÁÑ∂Âêé‰Ω†Â∞±ÊÉ≥ÊîæÂºÉÊú¨Ê¨°ÂÆâË£ÖÔºåÊúâÁßçÊó•‰∫ÜdogÁöÑÊÑüËßâÂêß
‰∏çËøáÊ≤°ÊúâÂÖ≥Á≥ªÔºåÂè™Ë¶Å‰Ω†Á®çÂæÆ‰∫ÜËß£‰∏ãAnaconda,Â∞±ËÉΩËß£ÂÜ≥‰Ω†Ë¶Å‰ΩøÁî®python3.9ÁöÑÁõÆÊ†á‰∫ÜÔºõ

### ÂÆâË£ÖÊ≠•È™§ÔºöÔºàÊúÄÂ•Ω‰∏çË¶ÅÂÆâË£ÖÂú®CÁõòÔºåÂÆâË£ÖÁ®ãÂ∫èÂ∞ΩÈáè‰∏çË¶ÅÂèàÁ©∫Ê†ºÁöÑÁõÆÂΩïÔºâ
1.ÂÆâË£ÖAnaconda3ÔºåËá™Ë°åÂéªÂÆòÁΩë‰∏ãËΩΩÂÆâË£ÖÔºåÁÑ∂ÂêéÂèØ‰ª•ËèúÂçïÈÄöËøáÂÆÉÁöÑpowerShell PromptÔºå‰πüÂèØ‰ª•Áõ¥Êé•cmdÔºåÁÑ∂ÂêécdÂà∞‰Ω†ÈúÄË¶ÅÂÆâË£ÖËØ•Á®ãÂ∫èÁöÑÁõÆÂΩï‰∏ãÔºåÊàëÁöÑÊòØH:\AIÁõÆÂΩï
2.ÂàõÂª∫ÁéØÂ¢É
```
#ÂàõÂª∫ËôöÊãüÁéØÂ¢É
conda create --name tpsm python=3.9 -y
#ÂàáÊç¢Âà∞ËôöÊãüÁéØÂ¢É
conda activate tpsm
‰∏ãËΩΩÊ®°ÂûãÁ®ãÂ∫è

git clone https://github.com/ym6789/tpsm.git tpsm
cd tpsm
```

3.ÂÆâË£ÖPyTorch(Êàë‰πü‰∏çÁü•ÈÅìËøôÊòØÂï•ÔºåÂèçÊ≠£ÂêéÈù¢Ëøô‰∏™ÊòØÊúÄÊäòËÖæ‰∫∫ÁöÑ)
 ```
 conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y
ÔºàËøôÈáåÊàë‰πüÊêû‰∏çÊáÇÔºåÊàëËøôËæπÁöÑcudaÁâàÊú¨ÊòØ12.3,‰πü‰∏çÁü•ÈÅìÊòØ‰∏çÊòØËøôÈáåÂá∫ÈóÆÈ¢ò‰∫ÜÔºâ

ÂÆâË£ÖÂÆåÊàêÂêéÔºåÊàëÊü•Áúã‰∏ãÊòØËøôÊ†∑ÁöÑÔºö
(tpsm) PS H:\AI\tpsm> pip list
Package            Version
------------------ ----------
Brotli             1.0.9
certifi            2023.11.17
cffi               1.16.0
charset-normalizer 2.0.4
cryptography       41.0.7
filelock           3.13.1
gmpy2              2.1.2
idna               3.4
Jinja2             3.1.2
MarkupSafe         2.1.3
mkl-fft            1.3.8
mkl-random         1.2.4
mkl-service        2.4.0
mpmath             1.3.0
networkx           3.1
numpy              1.26.3
Pillow             10.0.1
pip                23.3.1
pycparser          2.21
pyOpenSSL          23.2.0
PySocks            1.7.1
PyYAML             6.0.1
requests           2.31.0
setuptools         68.2.2
sympy              1.12
torch              2.1.2
torchaudio         2.1.2
torchvision        0.16.2
typing_extensions  4.9.0
urllib3            1.26.18
wheel              0.41.2
win-inet-pton      1.1.0




ÂÆâË£Örequirements.txtÊñá‰ª∂ÁöÑ‰æùËµñÁªÑ‰ª∂
(tpsm) PS H:\AI\tpsm> pip install -r requirements.txt

(tpsm) PS H:\AI\tpsm> pip list
Package            Version
------------------ ----------
Brotli             1.0.9
certifi            2023.11.17
cffi               1.14.6
charset-normalizer 2.0.4
colorama           0.4.6
cryptography       41.0.7
cycler             0.10.0
decorator          5.1.0
face-alignment     1.3.5
filelock           3.13.1
fsspec             2023.12.2
gmpy2              2.1.2
idna               3.4
imageio            2.9.0
imageio-ffmpeg     0.4.5
Jinja2             3.1.2
joblib             1.3.2
kiwisolver         1.3.2
llvmlite           0.39.1
MarkupSafe         2.1.3
matplotlib         3.4.3
mkl-fft            1.3.8
mkl-random         1.2.4
mkl-service        2.4.0
mpmath             1.3.0
networkx           2.6.3
numba              0.56.4
numpy              1.20.3
opencv-python      4.9.0.80
pandas             1.3.3
Pillow             9.2.0
pip                23.3.1
pycparser          2.20
pyOpenSSL          23.2.0
pyparsing          2.4.7
PySocks            1.7.1
python-dateutil    2.8.2
pytz               2021.1
PyWavelets         1.1.1
PyYAML             5.4.1
requests           2.31.0
scikit-image       0.18.3
scikit-learn       1.0
scipy              1.7.1
setuptools         68.2.2
six                1.16.0
sympy              1.12
threadpoolctl      3.2.0
tifffile           2023.12.9
torch              2.1.2
torchaudio         2.1.2
torchvision        0.16.2
tqdm               4.62.3
typing_extensions  4.9.0
urllib3            1.26.18
wheel              0.41.2
win-inet-pton      1.1.0
(tpsm) PS H:\AI\tpsm>




```
# ÊµãËØïpytorchÂÆâË£ÖÊòØÂê¶ÊàêÂäüÔºö
```
import torch

print(torch.__version__)

ËæìÂá∫Ôºö2.1.2ÔºàËøô‰∏™ÁâàÊú¨ÂêéÁª≠ÂèØËÉΩ‰ºöÂèà‰∏ÄÂ§ßÂ†ÜÁöÑÊä•ÈîôÔºåÈÇ£‰πüÊ≤°ÂÖ≥Á≥ªÔºåÂÖàÁªßÁª≠ÂÆûÊñΩÁúãÔºâ

```

### ‰∏ãËΩΩÊ®°ÂûãÊï∞ÊçÆÈõÜ Datasets
tips:ÂÖ∑‰ΩìÊØè‰∏Ä‰∏™Êï∞ÊçÆÈõÜ‰∏ªË¶ÅÂÅèÂêë‰∏éÈÇ£‰∏™È¢ÜÂüüÔºåÊú™Á†îÁ©∂

1) **MGif**. Follow [Monkey-Net](https://github.com/AliaksandrSiarohin/monkey-net).

2) **TaiChiHD** and **VoxCeleb**. Follow instructions from [video-preprocessing](https://github.com/AliaksandrSiarohin/video-preprocessing). 

3) **TED-talks**. Follow instructions from [MRAA](https://github.com/snap-research/articulated-animation).

Here are **VoxCeleb**, **TaiChiHD** and **TED-talks**  pre-processed datasets used in the paper. [Baidu Yun](https://pan.baidu.com/s/1HKJOtXBIiP_tlLiFbzn3oA?pwd=x7xv)
Download all files under the folder, then merge the files and decompress, for example:
```bash

cat vox.tar.* > vox.tar
tar xvf vox.tar

‰∏ãËΩΩÂêéÔºåÊääÊñá‰ª∂Ëß£ÂéãÂà∞H:\AI\tpsm\checkpoints

(tpsm) PS H:\AI\tpsm\checkpoints> dir
Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a----         2024/1/12     20:26     1359827700 drive-download-20240112T122126Z-001.zip
------          2022/5/2     16:27      306156401 mgif.pth.tar
------          2022/5/2     16:27      350993469 taichi.pth.tar
------          2022/5/2     16:27      350993469 ted.pth.tar
------          2023/7/8      2:04         667037 Thin-Plate-Spline-Motion-Model.ipynb
------          2022/5/2     16:27      350993469 vox.pth.tar


```
# Á¥†ÊùêÂáÜÂ§áÔºöÊâæ‰∏Ä‰∏™Êüê‰∫∫ÁöÑ‰∏ÄÂØ∏Â§¥ÂÉèÁÖßÁâá(png)ÔºåÊîæÂú®assetsÊñá‰ª∂Â§π‰∏ãÔºåÊõøÊç¢source.pngÊñá‰ª∂

# ÂºÄÂßãÊâßË°åÂëΩ‰ª§ÔºöÔºàÂà∞ËøôÈáåÂêéÔºåÂ∫îËØ•‰ºöËßâÂæóÊä•ÈîôÔºåËøô‰∏™ÊòØÂÆâË£ÖÂ§ßÈáèÁöÑgithubÈ°πÁõÆÂëäËØâÊàëÁöÑÁªèÈ™åÔºåËôΩÁÑ∂ËøòÊ≤°ÊúâÊâßË°åÔºâ
```
python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image assets/source.png --driving_video assets/driving.mp4
```

```
H:\AI\Anaconda3\envs\tpsm\python.exe: can't open file 'H:\AI\tpsm\checkpoints\demo.py': [Errno 2] No such file or directory
(tpsm) PS H:\AI\tpsm\checkpoints> cd ..
(tpsm) PS H:\AI\tpsm> python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image assets/source.png --driving_video assets/driving.mp4
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torch\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\TensorShape.cpp:3527.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
  0%|                                                                                                                                                                                                               | 0/169 [00:00<?, ?it/s]AttributeError: 'NoneType' object has no attribute 'close'
Exception ignored in: 'scipy.spatial.qhull._Qhull.__dealloc__'
Traceback (most recent call last):
  File "H:\AI\tpsm\demo.py", line 25, in relative_kp
    source_area = ConvexHull(kp_source['fg_kp'][0].data.cpu().numpy()).volume
AttributeError: 'NoneType' object has no attribute 'close'
  0%|                                                                                                                                                                                                               | 0/169 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "H:\AI\tpsm\demo.py", line 179, in <module>
    predictions = make_animation(source_image, driving_video, inpainting, kp_detector, dense_motion_network, avd_network, device = device, mode = opt.mode)
  File "H:\AI\tpsm\demo.py", line 86, in make_animation
    kp_norm = relative_kp(kp_source=kp_source, kp_driving=kp_driving,
  File "H:\AI\tpsm\demo.py", line 25, in relative_kp
    source_area = ConvexHull(kp_source['fg_kp'][0].data.cpu().numpy()).volume
  File "qhull.pyx", line 2434, in scipy.spatial.qhull.ConvexHull.__init__
  File "qhull.pyx", line 269, in scipy.spatial.qhull._Qhull.__init__
  File "messagestream.pyx", line 36, in scipy._lib.messagestream.MessageStream.__init__
OSError: Failed to open file b'C:\\Users\\\xe5\x8d\x8e\xe5\xb8\x88\xe7\xa7\x91\xe6\x95\x99\\AppData\\Local\\Temp\\scipy-hh_y9mwf'

(tpsm) PS H:\AI\tpsm>
Êü•ËØ¢‰∫Ü‰∏ãÔºåÁÑ∂ÂêéÂëäÁü•ËØ¥Âõ†‰∏∫Êúâ‰∏≠ÊñáË∑ØÂæÑÔºå‰øÆÊîπ‰∫ÜÁéØÂ¢ÉÂèòÈáè‰∏≠ÁöÑtempÔºåÊâÄ‰ª•ÂèÇËÄÉÈìæÊé•https://blog.csdn.net/datao3022/article/details/109186403
Âè≥ÈîÆÁÇπÂáªËÆ°ÁÆóÊú∫ -> Â±ûÊÄß -> È´òÁ∫ßÁ≥ªÁªüËÆæÁΩÆ -> ÁéØÂ¢ÉÂèòÈáè

ÂÖ≥Èó≠ÂëΩ‰ª§Á™óÂè£ÈáçÂêØÔºàÂ¶ÇÊûú‰∏çË°åÔºåÂ∞±ÈáçÂêØÁîµËÑëÂÜçËØïËØïÔºâ
```
```
ÁªßÁª≠ÊâßË°åÔºö
conda activate tpsm
python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image assets/source.png --driving_video assets/driving.mp4

H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
H:\AI\Anaconda3\envs\tpsm\lib\site-packages\torch\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\TensorShape.cpp:3527.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 169/169 [00:12<00:00, 13.58it/s]


Âà∞‰∫ÜËøôÈáåÔºåÊâßË°åÂÆåÊàêÔºÅÔºÅÔºÅ
(tpsm) PS H:\AI\tpsm>
Êü•ÁúãË∑ØÂæÑÔºö‚Ä™H:\AI\tpsm\result.mp4 ÔºåËøô‰∏™ÊòØÊñ∞ÁîüÊàêÁöÑËßÜÈ¢ë
ÁÑ∂ÂêéÊâìÂºÄÊí≠ÊîæÔºåÂèëÁé∞ÂõæÁâáÊòØÂ§¥ÂÉèÊòØÊõøÊç¢‰∫ÜÔºå‰ΩÜÊòØËßÜÈ¢ëÁöÑÂ£∞Èü≥Ê≤°Êúâ‰∫Ü
Ëµ∑Á†ÅÊù•ËØ¥ÔºåÁ®ãÂ∫èÊòØÊâßË°å‰∫ÜÔºåÊäΩÁ©∫Âú®Êù•ÊâßË°åÂêßÔºåÂ∏åÊúõÂ§ßÊãøÊù•ÊåáÂÆöÊåáÂØºÔºåÂ§ßÂÆ∂‰∏ÄËµ∑ÂÖ±ÂêåÂ≠¶‰π†Â≠¶‰π†

‰∏∫Ê≠§ËøòÁâπÊÑèËä±‰∫Ü6000ÂàÄÈÖçÁΩÆ‰∫Ü‰∏ÄÂè∞ÁîµËÑë~~~


```






# [CVPR2022] Thin-Plate Spline Motion Model for Image Animation

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
![stars](https://img.shields.io/github/stars/yoyo-nb/Thin-Plate-Spline-Motion-Model.svg?style=flat)
![GitHub repo size](https://img.shields.io/github/repo-size/yoyo-nb/Thin-Plate-Spline-Motion-Model.svg)

Source code of the CVPR'2022 paper "Thin-Plate Spline Motion Model for Image Animation"

[**Paper**](https://arxiv.org/abs/2203.14367) **|** [**Supp**](https://cloud.tsinghua.edu.cn/f/f7b8573bb5b04583949f/?dl=1)

### Example animation

![vox](assets/vox.gif)
![ted](assets/ted.gif)

**PS**: The paper trains the model for 100 epochs for a fair comparison. You can use more data and train for more epochs to get better performance.


### Web demo for animation
- Integrated into [Huggingface Spaces ü§ó](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)
- Try the web demo for animation here: [![Replicate](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model/badge)](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model)
- Google Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DREfdpnaBhqISg0fuQlAAIwyGVn1loH_?usp=sharing)

### Pre-trained models
- ~~[Tsinghua Cloud](https://cloud.tsinghua.edu.cn/d/30ab8765da364fefa101/)~~
- [Yandex](https://disk.yandex.com/d/bWopgbGj1ZUV1w)
- [Google Drive](https://drive.google.com/drive/folders/1pNDo1ODQIb5HVObRtCmubqJikmR7VVLT?usp=sharing)
- [Baidu Yun](https://pan.baidu.com/s/1hnXmDpIbRC6WqE3tF9c5QA?pwd=1234)

### Installation

We support ```python3```.(Recommended version is Python 3.9).
To install the dependencies run:
```bash
pip install -r requirements.txt
```


### YAML configs
 
There are several configuration files one for each `dataset` in the `config` folder named as ```config/dataset_name.yaml```. 

See description of the parameters in the ```config/taichi-256.yaml```.

### Datasets

1) **MGif**. Follow [Monkey-Net](https://github.com/AliaksandrSiarohin/monkey-net).

2) **TaiChiHD** and **VoxCeleb**. Follow instructions from [video-preprocessing](https://github.com/AliaksandrSiarohin/video-preprocessing). 

3) **TED-talks**. Follow instructions from [MRAA](https://github.com/snap-research/articulated-animation).

Here are **VoxCeleb**, **TaiChiHD** and **TED-talks**  pre-processed datasets used in the paper. [Baidu Yun](https://pan.baidu.com/s/1HKJOtXBIiP_tlLiFbzn3oA?pwd=x7xv)
Download all files under the folder, then merge the files and decompress, for example:
```bash
cat vox.tar.* > vox.tar
tar xvf vox.tar
```


### Training
To train a model on specific dataset run:
```
CUDA_VISIBLE_DEVICES=0,1 python run.py --config config/dataset_name.yaml --device_ids 0,1
```
A log folder named after the timestamp will be created. Checkpoints, loss values, reconstruction results will be saved to this folder.


#### Training AVD network
To train a model on specific dataset run:
```
CUDA_VISIBLE_DEVICES=0 python run.py --mode train_avd --checkpoint '{checkpoint_folder}/checkpoint.pth.tar' --config config/dataset_name.yaml
```
Checkpoints, loss values, reconstruction results will be saved to `{checkpoint_folder}`.



### Evaluation on video reconstruction

To evaluate the reconstruction performance run:
```
CUDA_VISIBLE_DEVICES=0 python run.py --mode reconstruction --config config/dataset_name.yaml --checkpoint '{checkpoint_folder}/checkpoint.pth.tar'
```
The `reconstruction` subfolder will be created in `{checkpoint_folder}`.
The generated video will be stored to this folder, also generated videos will be stored in ```png``` subfolder in loss-less '.png' format for evaluation.
To compute metrics, follow instructions from [pose-evaluation](https://github.com/AliaksandrSiarohin/pose-evaluation).


### Image animation demo
- notebook: `demo.ipynb`, edit the config cell and run for image animation.
- python:
```bash
CUDA_VISIBLE_DEVICES=0 python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image ./source.jpg --driving_video ./driving.mp4
```

# Acknowledgments
The main code is based upon [FOMM](https://github.com/AliaksandrSiarohin/first-order-model) and [MRAA](https://github.com/snap-research/articulated-animation)

Thanks for the excellent works!

And Thanks to:

- [@chenxwh](https://github.com/chenxwh): Add Web Demo & Docker environment [![Replicate](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model/badge)](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model) 

- [@TalkUHulk](https://github.com/TalkUHulk): The C++/Python demo is provided in [Image-Animation-Turbo-Boost](https://github.com/TalkUHulk/Image-Animation-Turbo-Boost)

- [@AK391](https://github.com/AK391): Add huggingface web demo [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)
