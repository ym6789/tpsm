é¦–å…ˆï¼Œå…ˆç¥ˆæ±‚è‡ªå·±ä¸€æ¬¡æ€§éƒ¨ç½²æˆåŠŸï¼(ä½†æ˜¯å¾€å¾€äº‹ä¸Žæ„¿è¿ï¼Œåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œä¸€å¤§å †çš„çŽ¯å¢ƒåŠä»¥æ¥ç¨‹åºçš„å®‰è£…ï¼Œèƒ½è®©ä½ ç–¯åˆ°å§¥å§¥å®¶ï¼Œä¸ä¿¡ä½ è·Ÿç€æˆ‘ä¸€èµ·è¯•è¯•ï¼‰
#æœ¬éƒ¨ç½²å‚è€ƒï¼š
https://github.com/natlamir/tpsm
https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model

#å®žè·µæ­¥éª¤ï¼š
1.æ‡‚å¾—ä¸Šç½‘ï¼šå¦‚æžœä¸æ‡‚ä¸Šç½‘ï¼Œä½ ä¼šæ›´åŠ çš„æŠ“ç‹‚ï¼Œå› ä¸ºä½ çš„å¤§é‡ä¾èµ–åº“ï¼Œéƒ½æ˜¯ä¼šè¢«å‘ŠçŸ¥æ— æ³•ä¸‹è½½ï¼Œå“ˆå“ˆå“ˆå“ˆ~~~ï¼Œå½“ç„¶ï¼Œèƒ½åŽŸç”Ÿçœ‹åˆ°æœ¬æ–‡ä»¶çš„äººï¼Œåº”è¯¥éƒ½æ‡‚å¾—ä¸Šç½‘äº†å§
2.ç¡¬ä»¶åŸºç¡€ï¼šéœ€è¦æœ‰ä¸€å¼ Nç³»åˆ—æ˜¾å¡ï¼Œæœ¬äººç”¨äºŽæµ‹è¯•çš„æ˜¯RTX3060 12Gçš„æ˜¾å¡
3.é©±åŠ¨è½¯ä»¶ä¾èµ–ï¼šå®‰è£…æ˜¾å¡é©±åŠ¨å’Œå·¥å…·åŒ…ï¼šCUDAï¼Œè‡ªå®šåœ¨ https://www.nvidia.cn/Download/Find.aspx?lang=cn æ ¹æ®å®žé™…ä¸‹è½½å®‰è£…
4.å¤šè™šæ‹ŸçŽ¯å¢ƒé…ç½®ç®¡ç†å·¥å…·Anaconda3ï¼šç”¨æˆ·Anaconda(condaå‘½ä»¤)æ¥åˆ›å»ºä¸€ä¸ªè™šæ‹ŸçŽ¯å¢ƒï¼Œå¯ä»¥ç»™è¿™ä¸ªè™šæ‹ŸçŽ¯å¢ƒå‘½åï¼Œåˆ‡æ¢åˆ°è¯¥è™šæ‹ŸçŽ¯å¢ƒåŽï¼Œç„¶åŽåœ¨è™šæ‹ŸçŽ¯å¢ƒå®‰è£…æ‰€æœ‰çš„ç¨‹åºä»¥åŠå¯¹åº”çš„ä¾èµ–ï¼Œåœ¨è¿è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°±ä¼šè‡ªç„¶å¼•ç”¨è¯¥è™šæ‹ŸçŽ¯å¢ƒä¸‹çš„å˜é‡ã€å‘½ä»¤å’Œé…ç½®ï¼Œå®Œç¾Žè§£å†³ä¸€å°ç³»ç»Ÿä¸‹å¯ä»¥å¤šä¸ªä¸åŒçŽ¯å¢ƒçš„ç›®æ ‡ï¼›


Anaconda3ï¼Œå…·æœ‰ä¸€å°ç³»ç»Ÿæ­å»ºå¤šä¸ªä¸æ‡‚è¿è¡ŒçŽ¯å¢ƒçš„éœ€æ±‚ï¼Œæ¯”å¦‚ä½ ç”µè„‘é‡ŒåŽŸæ¥è¿è¡Œpython2çš„ç¨‹åºï¼ŒçŽ°åœ¨éœ€è¦å®‰è£…python3.9æ¥è¿è¡Œæœ¬ç¨‹åºï¼Œé‚£ä¹ˆä½ åœ¨ä¸æ‡‚condaç®¡ç†çš„æ—¶å€™ï¼Œç¬¬ä¸€æƒ³æ³•æ˜¯å§python2ç»™å‡çº§ä¸ºpython3.9,ç„¶åŽä½ å¯èƒ½è¦æŠ“ç‹‚çš„æ˜¯ï¼Œå½“ä½ å®‰è£…å®Œæˆpython3.9åŽï¼Œå‘çŽ°åŽŸæ¥python2çš„ç¨‹åºï¼Œéƒ½æ— æ³•æ­£å¸¸è¿è¡Œäº†ï¼Œç„¶åŽä½ åˆç»§ç»­æŠŠpython3.9ç»™å¸è½½é‡æ–°å®‰è£…python2,ç„¶åŽä½ å°±æ²¡æœ‰ç„¶åŽï¼Œç„¶åŽä½ å°±æƒ³æ”¾å¼ƒæœ¬æ¬¡å®‰è£…ï¼Œæœ‰ç§æ—¥äº†dogçš„æ„Ÿè§‰å§
ä¸è¿‡æ²¡æœ‰å…³ç³»ï¼Œåªè¦ä½ ç¨å¾®äº†è§£ä¸‹Anaconda,å°±èƒ½è§£å†³ä½ è¦ä½¿ç”¨python3.9çš„ç›®æ ‡äº†ï¼›




# [CVPR2022] Thin-Plate Spline Motion Model for Image Animation

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
![stars](https://img.shields.io/github/stars/yoyo-nb/Thin-Plate-Spline-Motion-Model.svg?style=flat)
![GitHub repo size](https://img.shields.io/github/repo-size/yoyo-nb/Thin-Plate-Spline-Motion-Model.svg)

Source code of the CVPR'2022 paper "Thin-Plate Spline Motion Model for Image Animation"

[**Paper**](https://arxiv.org/abs/2203.14367) **|** [**Supp**](https://cloud.tsinghua.edu.cn/f/f7b8573bb5b04583949f/?dl=1)

### Example animation

![vox](assets/vox.gif)
![ted](assets/ted.gif)

**PS**: The paper trains the model for 100 epochs for a fair comparison. You can use more data and train for more epochs to get better performance.


### Web demo for animation
- Integrated into [Huggingface Spaces ðŸ¤—](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)
- Try the web demo for animation here: [![Replicate](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model/badge)](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model)
- Google Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DREfdpnaBhqISg0fuQlAAIwyGVn1loH_?usp=sharing)

### Pre-trained models
- ~~[Tsinghua Cloud](https://cloud.tsinghua.edu.cn/d/30ab8765da364fefa101/)~~
- [Yandex](https://disk.yandex.com/d/bWopgbGj1ZUV1w)
- [Google Drive](https://drive.google.com/drive/folders/1pNDo1ODQIb5HVObRtCmubqJikmR7VVLT?usp=sharing)
- [Baidu Yun](https://pan.baidu.com/s/1hnXmDpIbRC6WqE3tF9c5QA?pwd=1234)

### Installation

We support ```python3```.(Recommended version is Python 3.9).
To install the dependencies run:
```bash
pip install -r requirements.txt
```


### YAML configs
 
There are several configuration files one for each `dataset` in the `config` folder named as ```config/dataset_name.yaml```. 

See description of the parameters in the ```config/taichi-256.yaml```.

### Datasets

1) **MGif**. Follow [Monkey-Net](https://github.com/AliaksandrSiarohin/monkey-net).

2) **TaiChiHD** and **VoxCeleb**. Follow instructions from [video-preprocessing](https://github.com/AliaksandrSiarohin/video-preprocessing). 

3) **TED-talks**. Follow instructions from [MRAA](https://github.com/snap-research/articulated-animation).

Here are **VoxCeleb**, **TaiChiHD** and **TED-talks**  pre-processed datasets used in the paper. [Baidu Yun](https://pan.baidu.com/s/1HKJOtXBIiP_tlLiFbzn3oA?pwd=x7xv)
Download all files under the folder, then merge the files and decompress, for example:
```bash
cat vox.tar.* > vox.tar
tar xvf vox.tar
```


### Training
To train a model on specific dataset run:
```
CUDA_VISIBLE_DEVICES=0,1 python run.py --config config/dataset_name.yaml --device_ids 0,1
```
A log folder named after the timestamp will be created. Checkpoints, loss values, reconstruction results will be saved to this folder.


#### Training AVD network
To train a model on specific dataset run:
```
CUDA_VISIBLE_DEVICES=0 python run.py --mode train_avd --checkpoint '{checkpoint_folder}/checkpoint.pth.tar' --config config/dataset_name.yaml
```
Checkpoints, loss values, reconstruction results will be saved to `{checkpoint_folder}`.



### Evaluation on video reconstruction

To evaluate the reconstruction performance run:
```
CUDA_VISIBLE_DEVICES=0 python run.py --mode reconstruction --config config/dataset_name.yaml --checkpoint '{checkpoint_folder}/checkpoint.pth.tar'
```
The `reconstruction` subfolder will be created in `{checkpoint_folder}`.
The generated video will be stored to this folder, also generated videos will be stored in ```png``` subfolder in loss-less '.png' format for evaluation.
To compute metrics, follow instructions from [pose-evaluation](https://github.com/AliaksandrSiarohin/pose-evaluation).


### Image animation demo
- notebook: `demo.ipynb`, edit the config cell and run for image animation.
- python:
```bash
CUDA_VISIBLE_DEVICES=0 python demo.py --config config/vox-256.yaml --checkpoint checkpoints/vox.pth.tar --source_image ./source.jpg --driving_video ./driving.mp4
```

# Acknowledgments
The main code is based upon [FOMM](https://github.com/AliaksandrSiarohin/first-order-model) and [MRAA](https://github.com/snap-research/articulated-animation)

Thanks for the excellent works!

And Thanks to:

- [@chenxwh](https://github.com/chenxwh): Add Web Demo & Docker environment [![Replicate](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model/badge)](https://replicate.com/yoyo-nb/thin-plate-spline-motion-model) 

- [@TalkUHulk](https://github.com/TalkUHulk): The C++/Python demo is provided in [Image-Animation-Turbo-Boost](https://github.com/TalkUHulk/Image-Animation-Turbo-Boost)

- [@AK391](https://github.com/AK391): Add huggingface web demo [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)
